# bigdata-spark-sql
# Table des mati√®res

1. üöÄ [Spark SQL](#spark-sql) 
    - üìö [D√©finition](#d√©finition)
    - üîß [Caract√©ristiques Principales](#caract√©ristiques-principales) 
    - üéØ [Cas d'utilisation](#cas-dutilisation) 

2. üõ†Ô∏è [D√©pendances Spark dans le fichier pom.xml (Maven)](#d√©pendances-spark-dans-le-fichier-pomxml) 

3. üì± [Applications](#applications) 
    - üö® [Analyser les incidents](#analyser-les-incidents)
    - ‚öïÔ∏è [Consultation m√©dicale](#Consultationm√©dicale) 

4. üåü [Conclusion](#conclusion)




# üöÄSpark SQL
## üìöD√©finition

Spark SQL est un module Apache Spark con√ßu pour le traitement et l'analyse de donn√©es structur√©es √† l'aide du langage SQL. Il offre une interface unifi√©e permettant d'interagir avec des donn√©es structur√©es et semi-structur√©es, combinant les avantages du traitement distribu√© de Spark avec la familiarit√© du langage SQL pour l'analyse de donn√©es.

## üîßCaract√©ristiques Principales

1. **Traitement Distribu√© :** Spark SQL s'int√®gre parfaitement avec le mod√®le de traitement distribu√© de Spark, facilitant la gestion efficace de grands ensembles de donn√©es r√©partis sur un cluster.

2. **Langage SQL :** Les utilisateurs peuvent interagir avec Spark SQL en utilisant des requ√™tes SQL standards, simplifiant l'adoption par ceux qui sont familiers avec le langage SQL.

3. **Support pour les Formats de Donn√©es :** Spark SQL prend en charge divers formats tels que Parquet, Avro, JSON, CSV, etc., permettant aux utilisateurs de travailler avec diff√©rents types de donn√©es sans conversion complexe.

4. **Int√©gration avec Hive :** Spark SQL s'int√®gre nativement avec Apache Hive, offrant un acc√®s aux m√©tadonn√©es Hive, aux tables et aux op√©rations HiveQL, assurant une compatibilit√© avec les syst√®mes existants bas√©s sur Hive.

5. **Optimisations Catalyst :** Le moteur Catalyst est utilis√© pour effectuer des optimisations avanc√©es des requ√™tes, am√©liorant les performances du traitement en pr√©disant les pr√©dicats, en projetant des colonnes, etc.

6. **APIs DataFrame et Dataset :** En plus des requ√™tes SQL, Spark SQL offre des APIs bas√©es sur les concepts de DataFrame et de Dataset, permettant une expression programmatique des transformations de donn√©es dans divers langages.

## üéØCas d'utilisation

1. **Analyse de Donn√©es Structur√©es :** Spark SQL est id√©al pour l'analyse de grands ensembles de donn√©es structur√©es, permettant aux utilisateurs de tirer parti du langage SQL pour obtenir des insights.

2. **Migration depuis Hive :** Les organisations utilisant Hive peuvent migrer leurs requ√™tes et leurs donn√©es vers Spark SQL tout en continuant √† utiliser leurs scripts HiveQL existants.

3. **Int√©gration avec des Outils BI :** Gr√¢ce √† son support SQL standard, Spark SQL peut √™tre int√©gr√© avec des outils de business intelligence (BI) tels que Tableau, Power BI, etc., pour des analyses interactives.

# üõ†Ô∏èD√©pendances Spark dans le fichier pom.xml (Maven)

Ajouter cette d√©pendance dans votre fichier pom.xml
```
     <dependency>
         <groupId>org.apache.spark</groupId>
         <artifactId>spark-core_2.13</artifactId>
         <version>3.4.1</version>
     </dependency>
     <dependency>
         <groupId>org.apache.spark</groupId>
         <artifactId>spark-sql_2.13</artifactId>
         <version>3.4.1</version>
     </dependency>
```
# üì±Applications

## üö®Analyser les incidents 
Nous avons l'intention de cr√©er une application Spark destin√©e √† une entreprise industrielle charg√©e du traitement des incidents de chaque service.

Les donn√©es relatives aux incidents sont stock√©es dans un fichier au format CSV.

![Description de l'image](/sparkSql/fichierCsv.PNG)

- La classe IncidentsAnalysis:

```java
package ma.enset;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

import static org.apache.spark.sql.functions.*;

public class IncidentsAnalysis {
    public static void main(String[] args) {

        SparkSession spark = SparkSession.builder().appName("Incidents Analysis").master("local[*]").getOrCreate();

       //Dataset<Row> csvData = spark.read().option("header", true).option("inferSchema", true).csv("incidents_services_2023.csv");

        String file = "incidents_services_2023.csv";
        Dataset<Row> dataset = spark.read().csv(file).toDF("id", "titre", "description","service" ,"date" );
        dataset.printSchema();
        Dataset<Row> incidentsPerService = dataset.groupBy(col("service"))
                .agg(count("id").alias("number_of_incidents"))
                .orderBy(desc("number_of_incidents"));

        incidentsPerService.show();

        //dataset.select("date").show();
        Dataset<Row> datasetWithDate = dataset.withColumn("date", to_date(col("date"), "yyyy-MM-dd"));
        Dataset<Row> incidentsYear = datasetWithDate.withColumn("year", year(col("date")));
        Dataset<Row> incidentsPerYear = incidentsYear.groupBy("year").count().orderBy(col("count").desc());

        incidentsPerYear.show(2);
        spark.close();

    }
}
```
- Afficher le nombre d‚Äôincidents par service:
  
![Description de l'image](/sparkSql/incidentsCount.PNG)

## ‚öïÔ∏èConsultation m√©dicale

Cr√©ez une base de donn√©es MySQL nomm√©e DB_HOPITAL, qui contient trois tables :
1. **PATIENTS**

![Description de l'image](/sparkSql/Patients.PNG)
   
2. **MEDECINS**

![Description de l'image](/sparkSql/Medecin.PNG)

3. **CONSULTATIONS**

![Description de l'image](/sparkSql/Consultations.PNG)

***Obtenir l'acc√®s aux donn√©es de la base de donn√©es***
```java
        SparkSession ss = SparkSession.builder().appName("test mysql").master("local[*]").getOrCreate();
        Map<String , String > options = new HashMap< >( ) ;
        options.put( "driver" ,  "com.mysql.cj.jdbc.Driver" );
        options.put( "url" , "jdbc:mysql://localhost:3306/db_hopital" ) ;
        options.put( "user" , "root") ;
        options.put( "password" , "" ) ;

        Dataset<Row>  dataset = ss.read().format("jdbc")
                .options(options)
                .option("query" , "select * from MEDECINS ")
                .load();
        dataset.show();
```
1. ***Afficher le nombre de consultations par jour***
```java
 Dataset<Row>  dataset1 = ss.read().format("jdbc")
                .options(options)
                .option("query" , "SELECT date_consultation, COUNT(*) as nombre_de_consultations FROM CONSULTATIONS GROUP BY date_consultation ORDER BY date_consultation")
                .load();
        dataset1.show();
```

![Description de l'image](/sparkSql/consParJour.PNG)
   
2. ***Afficher le nombre de consultation par m√©decin. Le format d‚Äôaffichage est le suivant***

```NOM | PRENOM | NOMBRE DE CONSULTATION```

```java
  Dataset<Row>  dataset3 = ss.read().format("jdbc")
                .options(options)
                .option("query" , "SELECT M.NOM, M.PRENOM, COUNT(C.ID_MEDECIN) AS NOMBRE_DE_CONSULTATION\n" +
                        "FROM MEDECINS M\n" +
                        "JOIN CONSULTATIONS C ON M.ID = C.ID_MEDECIN\n" +
                        "GROUP BY M.ID\n" +
                        "ORDER BY NOMBRE_DE_CONSULTATION DESC\n")
                .load();
        dataset3.show();
```

![Description de l'image](/sparkSql/ConsParMed.PNG)

3. ***Afficher pour chaque m√©decin, le nombre de patients qu‚Äôil a assist√©***

```java
  Dataset<Row>  dataset4 = ss.read().format("jdbc")
                .options(options)
                .option("query" , "SELECT M.NOM, M.PRENOM, COUNT(DISTINCT C.ID_PATIENT) AS NOMBRE_DE_PATIENTS\n" +
                        "FROM MEDECINS M\n" +
                        "JOIN CONSULTATIONS C ON M.ID = C.ID_MEDECIN\n" +
                        "GROUP BY M.ID\n")
                .load();
        dataset4.show();
```

![Description de l'image](/sparkSql/NbrPatient.PNG)

# üåüConclusion

Spark SQL offre une solution puissante et polyvalente pour le traitement et l'analyse de donn√©es structur√©es dans des environnements distribu√©s. Son int√©gration transparente avec le reste de l'√©cosyst√®me Apache Spark, son support SQL, et ses fonctionnalit√©s avanc√©es en font un choix populaire pour les applications n√©cessitant une manipulation efficace de gros volumes de donn√©es structur√©es.
